---
title: "p8105_hw3_hq2163_hanbo"
author: "Hanbo Qiu"
date: "October 7, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

#Load the new package. 
library(tidyverse)
library(ggridges)
library(p8105.datasets)
```

##Problem 1

Create an overall_health dataset and focus on the “Overall Health” topic and organize responses as a factor taking levels from “Excellent” to “Poor”.

```{r}
data(brfss_smart2010)

overall_health = janitor::clean_names(brfss_smart2010) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE))
```

In 2002, which states were observed at 7 locations?

```{r}
#Make a table with distinct locations in 2002.
filter(overall_health, year == 2002) %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n == 7) %>% 
  knitr::kable(col.names = c("State", "No. of locations"))
```

The first column shows the states observed at 7 locations in 2002.

Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r}
distinct(overall_health, year, locationabbr, locationdesc) %>% 
  ggplot(aes(x = year, color = locationabbr)) +
  geom_freqpoly(binwidth = 1) +
  scale_x_continuous(breaks = 2002:2010, limits = c(2002,2010)) +
  labs(
    title = "The number of locations in each state from 2002 to 2010",
    x = "Year",
    y = "No. of locations",
    caption = "Data from the brfss_smart2010"
  ) +
  theme(legend.position="none")
```

The number of observations in State jumps to above 40 in 2007 and 2010. The number of observations all the other states kept under 20 for the rest of the years.

The mean and standard deviation of the proportion of “Excellent” responses across locations in NY State in 2002, 2006, and 2010.

```{r}
overall_health %>% 
  filter(year %in% c(2002, 2006, 2010), 
         response == "Excellent", 
         locationabbr == "NY") %>% 
  group_by(year) %>% 
  summarise(mean = mean(data_value, na.rm = TRUE),
            sd = sd(data_value, na.rm = TRUE)) %>% 
  knitr::kable(col.names = c("Year", "Mean of excellent proportion(%)", "SD of excellent proportion(%)"),
               digits = 2)
```

The mean and standard deviation of the proportion of “Excellent” responses across locations in NY State in 2002 is 24% and 4.49%, in 2006 is 22.5% and 4%, in 2010 is 22.7% and 3.57%.


For each year and state, compute the average proportion and make a five-panel plot

```{r}
select(overall_health, year, locationabbr, response, data_value) %>% 
  group_by(response, year, locationabbr) %>% 
  summarise(mean = mean(data_value, na.rm = TRUE)) %>%  
  ggplot(aes(x = year, y = mean, color = locationabbr)) +
  geom_line() +
  facet_grid(response ~., scales = "free_y")+
  labs(
    title = "Response distribution of state-level averages from 2002-2010",
    x = "Year",
    y = "The average proportion of each state in response category",
    caption = "Data from the brfss_smart2010"
  ) +
  theme(legend.position = "none")
```

##Problem 2

```{r}
# load the data from the p8105.datasets package and summarize the data
data(instacart)
summary(instacart)
```
A short description of the dataset: Instacart is an online grocery service that allows you to shop online from local stores. The dataset contains 1,384,617 observations and 15variables of 131,209 unique users, where each row in the dataset is an order about product information. Key variables includes 1."order_dow":the day of the week on which the order was placed. It can help us to analyse which kind of products are sold best at what time and then Instacart could get the info of promoting at that day 2.order_hour_of_day: the hour of the day on which the order was placed.Like we can analyze when to promote specific products 3.product_name: name of the product. 4.aisle: the name of the aisle

How many aisles are there, and which aisles are the most items ordered from?

```{r}
dist_aisle = distinct(instacart, aisle_id)
n_aisle = nrow(dist_aisle)

item_aisle = count(instacart, aisle_id, sort = T)
item_aisle[1,1]
```

There are `r n_aisle` aisles and `r item_aisle[1,1]`is the most items ordered from.


Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.



Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
three_aisle = filter(instacart, aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  count(product_name)

product_aisle = select(instacart, product_name, aisle) %>% 
  distinct(product_name, .keep_all = T)

com_aisle_product = left_join(three_aisle, product_aisle, by = "product_name") %>% 
  group_by(aisle) %>% 
  filter(min_rank(desc(n)) == 1) %>% 
  select(aisle, product_name) %>% 
  arrange(aisle)

names(com_aisle_product)[1:2] = c("Aisle", "Most popular product")
com_aisle_product

```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
apple_icecream = filter(instacart, product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean = round(mean(order_hour_of_day), 0)) %>% 
  spread(key = order_dow, value = mean )
```

##Problem 3

```{r}
# load the data from the p8105.datasets package and summarize the data
data(ny_noaa)

summary(ny_noaa)
```

A short description of the dataset: ny_noaa provides some weather data in NY city, including the GHCN -Daily database of summary statistics from weather stations. The dataset contains 2595176 observations and 7 variables, where each row in the dataset is daily record of weather during 1981-2010.
Key variables includes 1."date" is Date of observation 2."prcp" which represents Precipitation (tenths of mm) recorded by the weather station  on a specific day 3."snow" which represents Snowfall (mm) recorded by the weather station on a specific day 4."snwd": which represents Snow depth (mm) recorded by the weather station on a specific day 5."tmax" which represents the maximum temperature (tenths of degrees C) recorded by the weather station on a specific day 6."tmin" which represents the Minimum temperature (tenths of degrees C) recorded by the weather station on a specific day.
There are missing data in variables: "prcp"(145838 NA), "snow"(381221 NA), "snwd"(591786 NA) and ."tmax","tmin". Each weather station collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data which might influence the result.

Do some data cleaning and create separate variables for year, month, and day.

Convert observations for temperature, precipitation, and snowfall.
```{r}
ny_noaa_ymd = separate(ny_noaa, date, into = c("year", "month", "day"), sep = "-") %>% 
    mutate(prcp = prcp / 10, tmin = as.numeric(tmin) / 10, tmax = as.numeric(tmax) / 10)
```

For snowfall, what are the most commonly observed values? Why?

```{r}
snowfall_daycount = count(ny_noaa, snow, sort = T)
snowfall_daycount[1,1]
```

Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
temp_jan_jul = filter(ny_noaa_ymd, month %in% c("01","07")) %>% 
  group_by(id, year, month) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = T)) %>%
  ungroup() %>% 
  mutate(month = recode(month, "01" = "Jan", "07" = "Jul"))

ggplot(temp_jan_jul, aes(x = year, y = mean_tmax,fill = year)) +
  geom_violin(color = "blue", alpha = 0.5) +
  stat_summary(fun.y = median, geom = "point", color = "blue", size = 1) +
  facet_grid(month ~., scales = "free_y") +
  theme(legend.position = "none")
```